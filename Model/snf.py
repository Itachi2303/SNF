# -*- coding: utf-8 -*-
"""SNF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15jbQLqhfFg0702_O2gy-0oXndCGRACOP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle


#importing dataset
df=pd.read_csv('./content/diabetes.csv')
df.head()

df.shape

df.info()

df.describe()

sns.pairplot(df,hue='Outcome')

corr = df.corr()
corr.style.background_gradient(cmap='coolwarm')

sns.set_theme(style="whitegrid")
sns.boxplot(x="Age", data=df, palette="Set3")
plt.title("Age Distribution")

fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax)

df.Outcome.value_counts().plot(kind='bar')
plt.xlabel("Diabetes or Not")
plt.ylabel("Count")
plt.title("Outcome ")
#Here we can see that dataset is not much imbalanced so there is no need to balance.

X=df.drop('Outcome',axis=1)
X.head()

y=df['Outcome']
y.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=0)

X_train.shape

from sklearn.preprocessing import StandardScaler

sc_x=StandardScaler()
X_train=sc_x.fit_transform(X_train)
X_test=sc_x.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier(n_neighbors=5,metric='euclidean',p=2)
knn.fit(X_train,y_train)

pickle.dump(knn, open('savedmodel.pkl', 'wb'))

y_pred=knn.predict(X_test)
y_pred

knn.score(X_test,y_test)

from sklearn.metrics import accuracy_score
from sklearn import metrics

metrics.accuracy_score(y_test,y_pred)

from sklearn.metrics import confusion_matrix
mat = confusion_matrix(y_test, y_pred)
mat

from sklearn.metrics import classification_report
target_names = ['Diabetes', 'Normal']
print(classification_report(y_test, y_pred, target_names=target_names))

#For selecting K value
error_rate = []

# Will take some time
for i in range(1,40):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))

import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

#From graph we can see that optimize k value is 16,17,18
# Now we will train our KNN classifier with this k values

knn=KNeighborsClassifier(n_neighbors=18,metric='euclidean',p=2)
knn.fit(X_train,y_train)

y_pred=knn.predict(X_test)
y_pred

knn.score(X_test,y_test)

from sklearn.metrics import confusion_matrix
mat = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(mat, annot=True)

from sklearn.metrics import classification_report
target_names = ['Diabetes', 'Normal']
print(classification_report(y_test, y_pred, target_names=target_names))

